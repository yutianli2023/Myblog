{"meta":{"title":"Yutian Li","subtitle":"","description":"This is a blog","author":"yutianli","url":"http://example.com","root":"/"},"pages":[{"title":"","date":"2023-11-12T13:57:17.686Z","updated":"2023-11-12T13:57:17.686Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"下面写关于自己的内容"},{"title":"我的朋友们","date":"2023-11-12T14:12:35.187Z","updated":"2023-11-12T14:12:35.187Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2023-11-12T13:58:14.537Z","updated":"2023-11-12T13:58:14.537Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2023-11-12T13:58:39.851Z","updated":"2023-11-12T13:58:39.851Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Decoupled Knowledge Distillation","slug":"Decoupled-Knowledge-Distillation","date":"2023-11-18T07:31:11.000Z","updated":"2023-11-18T07:38:55.737Z","comments":true,"path":"2023/11/18/Decoupled-Knowledge-Distillation/","link":"","permalink":"http://example.com/2023/11/18/Decoupled-Knowledge-Distillation/","excerpt":"","text":"摘要 State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we reformulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the “difficulty” of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MSCOCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller. 笔记： 最先进的蒸馏方法主要基于从中间层提取深层特征，而对逻辑蒸馏&#x3D;&#x3D;logits蒸馏就是对输出进行蒸馏&#x3D;&#x3D;的重要性往往被大大忽视。 论文重新定义了经典的知识蒸馏损失函数为两部分：&#x3D;&#x3D;目标类别知识蒸馏（TCKD）和非目标类别知识蒸馏（NCKD）&#x3D;&#x3D;。 论文通过实证研究和证明了这两部分的影响：&#x3D;&#x3D;TCKD传递了关于训练样本“难度”的知识，而NCKD则是逻辑蒸馏有效的主要原因。&#x3D;&#x3D; 论文提出了&#x3D;&#x3D;Decoupled Knowledge Distillation (DKD)&#x3D;&#x3D;方法，使得TCKD和NCKD能够更高效、更灵活地发挥作用。 关键词：知识蒸馏、目标类别知识蒸馏、非目标类别知识蒸馏。 引言 在过去几十年中，深度神经网络（DNN）在计算机视觉领域引起了革命，成功地提升了各种真实场景任务，例如图像分类[9,13,21]、目标检测[8,27]和语义分割[31,45]。然而，强大的网络通常需要大容量的模型，从而引入了高计算和存储成本。在工业应用中，这些成本是不可取的，因此轻量级模型得到广泛部署。在文献中，一个潜在的降低成本的方向是知识蒸馏（KD）。知识蒸馏代表了一系列方法，专注于从一个复杂模型（教师）传递知识给一个轻量级模型（学生），这可以在不引入额外成本的情况下提升轻量级模型的性能。 知识蒸馏（KD）的概念最初是在[12]中提出的，通过最小化教师和学生之间的预测logits之间的KL散度来传递知识&#x3D;&#x3D;知识蒸馏的基本思想就是让学生学习老师的预测能力&#x3D;&#x3D;（图1a）。自从[28]以来，大部分的研究关注已经转向了从中间层的深层特征中提取知识&#x3D;&#x3D;中间层提取知识就是老师把思考过程给学生&#x3D;&#x3D;。与基于logits的方法相比，特征蒸馏的性能在各种任务上都更优秀，因此对于logit蒸馏的研究几乎没有被涉及。然而，基于特征的方法的训练成本不令人满意，因为在训练过程中引入了额外的计算和存储开销（例如，网络模块和复杂操作）来进行深层特征的蒸馏。&#x3D;&#x3D;基于特征的方法通过传递教师模型的特征信息来引导学生模型的训练，基于logits方法主要关注模型预测的logits（对数几率），并且基于logits的方法更直接地利用了模型输出的中间表示。&#x3D;&#x3D; 逻辑蒸馏在计算和存储成本方面要求较低，但性能相对较差。直观上，逻辑蒸馏应该能够达到与特征蒸馏相当的性能，因为logits比深层特征具有更高的语义级别。我们认为逻辑蒸馏的潜力受到未知原因的限制，导致性能不佳。为了振兴基于logits的方法，我们通过深入研究知识蒸馏的机制来开始这项工作。首先，我们将分类预测分为两个层次：（1）目标类别和所有非目标类别的二元预测&#x3D;&#x3D;目标类别与非目标类别进行二元预测，可以帮助学生模型更好地理解和学习目标类别的区分特征&#x3D;&#x3D;，以及（2）每个非目标类别的多类别预测&#x3D;&#x3D;对每个非目标类别进行多类别预测，可以帮助学生模型更好地理解和学习非目标类别之间的关系和区分特征。这样的设计可以进一步提高学生模型对非目标类别的预测性能，并有助于更全面地捕捉教师模型的知识&#x3D;&#x3D;。基于此，我们将经典的知识蒸馏损失函数[12]重新定义为两个部分，如图1b所示。一个部分是用于目标类别的二元logit蒸馏，另一个部分是用于非目标类别的多类别logit蒸馏。为了简化，我们分别将它们称为目标分类知识蒸馏（TCKD）和非目标分类知识蒸馏（NCKD）。这种重新定义使我们能够独立地研究这两个部分的影响。 TCKD通过二元logit蒸馏来传递知识，这意味着只提供了目标类别的预测，而对于每个非目标类别的具体预测是未知的。一个合理的假设是，TCKD传递了关于训练样本“hard”的知识，即描述了每个训练样本的识别难度。为了验证这一点，我们从三个方面设计了实验来增加训练数据的“难度”，即增强数据增强、加入噪声标签和使用固有具有挑战性的数据集。 NCKD仅考虑非目标logits之间的知识。有趣的是，我们通过实证证明，仅应用NCKD就能够达到与经典知识蒸馏相当甚至更好的结果，这表明非目标logits中所包含的知识非常重要，可能是关键的“暗知识”。&#x3D;&#x3D;这个应该比较常规，看上面这图&#x3D;&#x3D; 更重要的是，我们的重新定义表明经典的知识蒸馏损失函数是一种高度耦合的形式（如图1b所示），这可能是逻辑蒸馏潜力有限的原因。首先，NCKD损失项由一个与教师对目标类别的预测置信度呈负相关的系数加权。因此，较大的预测分数会导致较小的权重。这种耦合显著抑制了NCKD对预测良好的训练样本的影响。这种抑制是不可取的，因为教师对训练样本的信心越大，它提供的知识就越可靠和有价值。&#x3D;&#x3D;举个例子，就是老师认为上面的数字很有可能是3(知识)，但是数字3却分到了更少的权重&#x3D;&#x3D;其次，TCKD和NCKD的重要性是耦合的，即不允许分别对TCKD和NCKD进行加权。这种限制也是不可取的，因为TCKD和NCKD应该分别考虑，因为它们的贡献来自不同的方面。 为了解决这些问题，我们提出了一种灵活高效的逻辑蒸馏方法，名为Decoupled Knowledge Distillation（DKD，图1b）&#x3D;&#x3D;解耦知识蒸馏&#x3D;&#x3D;。DKD通过将与教师置信度负相关的系数替换为一个常数值&#x3D;&#x3D;对上面的改进，数字3应该获得更大的权重&#x3D;&#x3D;，将NCKD损失与之解耦，从而提高对预测良好样本的蒸馏效果。同时，NCKD和TCKD也被解耦，可以通过调整每个部分的权重来单独考虑它们的重要性。&#x3D;&#x3D;CVPR23有个动态调整的工作&#x3D;&#x3D; 总体而言，我们的贡献可以总结如下：• 我们通过将经典的知识蒸馏方法分为TCKD和NCKD提供了一种深入的视角来研究逻辑蒸馏。此外，我们分别分析并证明了两部分的影响。• 我们揭示了经典知识蒸馏损失函数由于高度耦合的形式而存在的局限性。将NCKD与教师置信度耦合会抑制知识传递的效果。将TCKD与NCKD耦合限制了平衡两个部分的灵活性。• 我们提出了一种有效的逻辑蒸馏方法，命名为DKD，以克服这些限制。DKD在各种任务上实现了最先进的性能。我们还通过实验证实了与基于特征的蒸馏方法相比，DKD具有更高的训练效率和更好的特征可迁移性。 相关工作知识蒸馏（Knowledge Distillation，简称KD）的概念最早是由Hinton等人在[12]中提出的。KD定义了一种学习方式，即利用更大的教师网络来指导较小的学生网络在多个任务上进行训练[12, 17, 18]。通过教师网络的软标签，”暗知识”被传递给学生网络。为了更关注负logits，引入了温度超参数。以下的研究可以分为两种类型：基于logits的蒸馏[3,6,22,40,44]和基于中间特征的蒸馏[10, 11, 14, 15, 23, 25, 28, 33, 34, 41, 43]。 先前的logits蒸馏方法主要集中在提出有效的正则化和优化方法，而不是新颖的方法。DML [44] 提出了一种互相学习的方式，同时训练学生和教师。TAKD [22] 引入了一个名为”教师助手”的中等规模网络来弥合教师和学生之间的差距。此外，还有一些工作专注于对经典KD方法的解释[2, 26]。 目前的最新方法主要基于中间特征，可以直接将教师网络的表示传递给学生[10, 11, 28]，或者将教师捕获的样本之间的相关性传递给学生[23, 33, 34]。大多数基于特征的方法可以取得更好的性能（显著高于基于logits的方法），但涉及相当高的计算和存储成本。 本文的重点是分析限制基于logits的方法潜力的因素，并振兴logits蒸馏方法 方法在这一部分中，我们深入研究知识蒸馏的机制。我们重新定义知识蒸馏损失，&#x3D;&#x3D;将其转化为两部分的加权和，一部分与目标类相关，另一部分与目标类无关&#x3D;&#x3D;。我们探索了每个部分在知识蒸馏框架中的影响，并揭示了经典知识蒸馏方法的一些局限性。在这些发现的启发下，我们进一步提出了一种新颖的logit蒸馏方法，在各种任务上取得了显著的性能表现。 重定义KD TCKD和NCKD的影响每个部分的性能提升。我们分别研究了在CIFAR-100 [16] 数据集上，TCKD 和 NCKD 对性能的影响。我们选择了 ResNet [9]、WideResNet (WRN) [42] 和 ShuffleNet [21] 作为训练模型，其中考虑了相同和不同的架构。实验结果如表1所示。对于每对教师-学生模型，我们报告了以下结果：(1) 学生基线（普通训练），(2) 经典KD（同时使用TCKD和NCKD），(3) 单独的TCKD，和(4) 单独的NCKD。每个损失的权重都设置为1.0（包括默认的交叉熵损失）。其他实现细节与第4节中相同。 直观地说，&#x3D;&#x3D;TCKD专注于与目标类相关的知识&#x3D;&#x3D;，因为相应的损失函数仅考虑二进制概率。相反，&#x3D;&#x3D;NCKD则关注非目标类之间的知识&#x3D;&#x3D;。我们注意到，仅应用TCKD可能没有帮助（例如，在ShuffleNet-V1上只有0.02%和0.12%的提升），甚至可能&#x3D;&#x3D;有害&#x3D;&#x3D;（例如，在WRN-16-2上下降了2.30%，在ResNet8×4上下降了3.87%）。然而，NCKD的蒸馏性能与经典KD相当甚至更好（例如，在ResNet8×4上为1.76%对比1.13%）。消融结果表明，与&#x3D;&#x3D;目标类相关的知识可能没有非目标类之间的知识重要&#x3D;&#x3D;。为了深入研究这一现象，我们提供以下进一步的分析。 TCKD传递了有关训练样本“难度”的知识。根据公式（5），TCKD通过二元分类任务传递了“暗知识”，这可能与样本的“难度”有关。例如，对于具有p^T^t &#x3D; 0.99的训练样本来说，相比较于p^T^t &#x3D; 0.75的另一个样本，学生更容易学习。由于TCKD传递了训练样本的“难度”，我们认为当训练数据变得具有挑战性时，其效果将得到体现。然而，CIFAR-100训练集很容易适应。因此，教师提供的“难度”知识并不具有信息量。在这部分中，我们从三个角度进行实验证实：训练数据越困难，TCKD提供的好处越多。 （1）应用强化增强是增加训练数据难度的一种直接方法。我们使用AutoAugment [5]在CIFAR-100上训练了一个ResNet32×4模型作为教师模型，达到81.29%的top-1验证准确率。对于学生模型，我们使用ResNet8×4和ShuffleNetV1模型进行训练，并进行了带有&#x2F;不带有TCKD的实验。表2中的结果显示，如果&#x3D;&#x3D;应用强化增强，TCKD可以获得显著的性能提升&#x3D;&#x3D;。 （2）噪声标签也可以增加训练数据的难度。我们使用ResNet32×4模型作为教师模型，ResNet8×4作为学生模型，在CIFAR-100数据集上使用{0.1, 0.2, 0.3}的对称噪声比率进行训练，遵循[7, 35]的方法。如表3所示，结果表明&#x3D;&#x3D;TCKD在噪声更大的训练数据上实现了更多的性能提升&#x3D;&#x3D;。 （3）我们还考虑了具有挑战性的数据集（例如ImageNet [29]）。表4显示，TCKD可以在ImageNet上带来+0.32%的性能提升。 综上所述，通过尝试各种策略来增加训练数据的难度（例如强化增强、噪声标签、困难任务），我们证明了TCKD的有效性。结果验证了在更具挑战性的训练数据上蒸馏知识时，与训练样本的“难度”相关的知识可能更有益处。 NCKD是为什么逻辑回归蒸馏有效但被大大抑制的主要原因。有趣的是，在表1中我们注意到，&#x3D;&#x3D;当仅应用NCKD时，性能与经典KD相当甚至更好。这表明非目标类之间的知识对于逻辑回归蒸馏至关重要，可能是显著的“暗知识”&#x3D;&#x3D;。然而，通过回顾公式（5），我们注意到NCKD损失与(1 − p^T^t)耦合在一起，其中p^T^t表示教师对目标类的置信度。因此，&#x3D;&#x3D;更高置信度的预测会导致较小的NCKD权重。&#x3D;&#x3D;我们推测，&#x3D;&#x3D;教师对训练样本的置信度越高，它提供的知识就越可靠和有价值。然而，这种置信度预测会大大抑制损失权重&#x3D;&#x3D;。我们推测，这个事实会限制知识转移的有效性，这是由我们在公式（5）中重新定义KD所首次调查的。 我们设计了一个消融实验来验证预测良好的样本确实传递了比其他样本更好的知识。首先，我们根据p^T^t对训练样本进行排序，并将其均匀分成两个子集。为了清晰起见，一个子集包括具有前50% p^T^t的样本，而其余样本则位于另一个子集中。然后，我们对每个子集上使用NCKD训练学生网络，以比较性能提升（同时交叉熵损失仍然在整个集合上）。表5显示，&#x3D;&#x3D;在前50%的样本上利用NCKD可以获得更好的性能，这表明预测良好的样本的知识比其他样本更丰富&#x3D;&#x3D;。然而，&#x3D;&#x3D;由于教师的高置信度，预测良好的样本的损失权重被抑制了&#x3D;&#x3D;。 解耦知识蒸馏到目前为止，我们已经将经典的KD损失重新定义为两个独立部分的加权和，并进一步验证了TCKD的有效性，并揭示了NCKD的抑制效应。具体而言，TCKD传递了关于训练样本“难度”的知识。在更具挑战性的训练数据上，TCKD可以获得更显著的改进。NCKD传递了非目标类之间的知识，在权重(1 − p^T^t)相对较小的情况下会被抑制。 直觉上，TCKD和NCKD都是不可或缺和至关重要的。然而，在经典的KD公式中，TCKD和NCKD从以下几个方面耦合在一起： 首先，NCKD与(1 − p^T^t)耦合在一起，这可能会抑制对预测良好的样本的NCKD。由于表5中的结果显示预测良好的样本可以带来更多的性能提升，这种耦合形式可能会限制NCKD的有效性。 另外，在经典的KD框架下，NCKD和TCKD的权重是耦合在一起的。不能改变每个项的权重以平衡其重要性。我们认为应该单独考虑TCKD和NCKD，因为它们的贡献来自不同的方面。 由于我们对KD的重新定义，我们提出了一种名为Decoupled Knowledge Distillation（DKD）的新型逻辑回归蒸馏方法，以解决上述问题。我们提出的DKD以解耦的形式独立考虑TCKD和NCKD，如图1b所示。具体而言，我们引入两个超参数α和β，作为TCKD和NCKD的权重。DKD的损失函数可以写成如下形式： 在DKD中，会抑制NCKD有效性的(1 − p^T^t)被β所替代。而且，可以调整α和β来平衡TCKD和NCKD的重要性。通过解耦NCKD和TCKD，DKD为逻辑回归蒸馏提供了一种高效灵活的方式。算法1以类似PyTorch [24]的风格提供了DKD的伪代码。 实验 主要在两个代表性任务上进行了实验，即图像分类和目标检测，包括以下内容： CIFAR-100：这是一个著名的图像分类数据集，包含100个类别的32×32像素图像。训练集和验证集分别包含50,000张和10,000张图像。 ImageNet：这是一个大规模分类数据集，包含1000个类别。训练集包含1.28百万张图像，验证集包含50,000张图像。 MS-COCO：这是一个包含80个类别的通用目标检测数据集。train2017分集包含118,000张图像，val2017分集包含5,000张图像。 由于篇幅限制，所有实现细节都附在补充材料中。 主要结果首先，我们展示了解耦(1)NCKD和p^T^t以及(2)NCKD和TCKD所带来的改进。然后，我们在图像分类和目标检测任务上对我们的方法进行了基准测试。 消融实验：α和β。下面的两个表格报告了在不同α和β下的学生准确率（%）。其中，ResNet32×4被设置为教师网络，ResNet8×4被设置为学生网络。首先，在第一个表格中，我们证明解耦(1 − p^T^t)和NCKD可以带来合理的性能提升（73.63%对74.79%）。然后，我们证明解耦NCKD和TCKD的权重可以进一步提升性能（74.79%对76.32%）。此外，第二个表格表明TCKD是不可或缺的，并且在不同α约为1.0的情况下，来自TCKD的改进是稳定的。 对于CIFAR-100图像分类任务，我们讨论了在该任务上的实验结果，以验证我们的DKD方法。验证准确率的结果如表格6和表格7所示。表格6包含了教师网络和学生网络具有相同网络架构的结果。而表格7展示了教师网络和学生网络来自不同系列的结果。 值得注意的是，与基准方法和经典的知识蒸馏方法相比，DKD在所有教师-学生配对上都取得了一致的改进。我们的方法在相同系列和不同系列的教师-学生配对上分别实现了1％至2％和2％至3％的性能提升。这强有力地支持了DKD的有效性。此外，DKD在特征为基础的蒸馏方法上实现了可比甚至更好的性能，显著提高了蒸馏性能和训练效率之间的权衡，这将在4.2节中进一步讨论。 在ImageNet图像分类任务中，我们报告了Top-1和Top-5的准确率，具体结果如表格8和表格9所示。我们的DKD取得了显著的改进。值得一提的是，DKD的性能优于特征蒸馏方法的大多数最新结果。 对于MS-COCO目标检测任务，正如之前的研究所讨论的，目标检测任务的性能在很大程度上取决于深层特征的质量，以定位感兴趣的目标物体。这个规则在目标检测器之间的知识迁移中也适用[17, 37]，即特征模仿对于目标定位至关重要，因为逻辑回归无法提供目标定位的知识。如表格10所示，仅应用DKD很难取得出色的性能，但预期地超过了经典的知识蒸馏方法。因此，我们引入了基于特征的蒸馏方法ReviewKD [1]来获得满意的结果。可以观察到，我们的DKD可以进一步提高AP指标，即使ReviewKD的蒸馏性能相对较高。综上所述，在目标检测任务中，将我们的DKD与基于特征的蒸馏方法相结合，可以获得新的最先进结果。 扩展为了更好地理解DKD，我们从四个方面提供了扩展说明。 首先，我们全面比较了DKD与代表性最先进方法的训练效率。然后，我们提供了一个新的观点来解释为什么更大的模型并不总是更好的教师，并通过利用DKD来缓解这个问题。此外，按照[33]的方法，我们检查了DKD学习的深层特征的可迁移性。我们还提供了一些可视化结果来验证DKD的优越性。 训练效率方面，我们评估了最先进蒸馏方法的训练成本，证明了DKD的高训练效率。如图2所示，我们的DKD在模型性能和训练成本（例如训练时间和额外参数）之间取得了最佳平衡。由于DKD是从经典的知识蒸馏方法演化而来，它几乎具有与KD相同的计算复杂度，而且当然没有额外的参数。然而，基于特征的蒸馏方法需要额外的训练时间来蒸馏中间层特征，并且会增加GPU内存的开销。 改善大模型的性能。我们提供了一个新的潜在解释，即为什么更大的模型并不总是更好的教师。具体而言，更大的教师模型预期能够传递更多有益的知识，但实际上并不能实现这一点，甚至比较小的教师模型性能更差。以往的研究[3, 36]解释了这一现象是由于大教师模型和小学生模型之间的容量差距较大。然而，我们认为主要原因是NCKD的抑制，即随着教师模型变得更大，(1−p^T^t)会变得更小。此外，与这个问题相关的工作也可以从这个角度解释，例如ESKD [3]采用提前停止的教师模型来缓解这个问题，这些教师模型在训练过程中会达到较低的收敛程度，并产生较小的p^T^t值。 为了验证我们的猜测，我们在一系列教师模型上进行了DKD实验。表格11和表格12中的实验结果一致表明，我们的DKD缓解了更大的模型并不总是更好的教师的问题。 特征可迁移性。我们进行了实验来评估深层特征的可迁移性，以验证我们的DKD传递了更具普遍性的知识。按照[33]的方法，我们使用从WRN-40-2蒸馏出的WRN-16-2作为特征提取器。然后，在下游数据集（如STL-10 [4]和Tiny-ImageNet6）上进行线性探测任务。结果如表格13所示，证明了我们的DKD学习到的特征具有出色的可迁移性。实现细节请参见补充材料。 可视化。我们从两个角度进行可视化展示（以ResNet32x4作为教师模型，以ResNet8x4作为学生模型在CIFAR-100数据集上）。第一，t-SNE（图3）的结果显示，DKD的表示比KD更具可分性，证明DKD有助于提高深层特征的可辨别性。第二，我们还可视化了学生模型和教师模型输出的logits之间的相关矩阵的差异（图4）。与KD相比，DKD有助于使学生输出更类似于教师的logits，即达到更好的蒸馏性能。 结论与讨论本文提供了一种新的视角来解释逻辑蒸馏，通过将经典的知识蒸馏损失重新定义为目标类知识蒸馏（TCKD）和非目标类知识蒸馏（NCKD）两部分。我们分别研究和证明了这两部分的效果。更重要的是，我们揭示了知识蒸馏中耦合形式的局限性，限制了知识传递的有效性和灵活性。为了克服这些问题，我们提出了解耦的知识蒸馏（DKD），在CIFAR-100、ImageNet和MS-COCO数据集上对图像分类和目标检测任务取得了显著的改进。此外，我们还展示了DKD在训练效率和特征可迁移性方面的优越性。希望本文对未来的逻辑蒸馏研究有所贡献。 局限性和未来工作。以下是明显的局限性讨论。DKD在目标检测任务上无法超过最先进的基于特征的方法（例如ReviewKD [1]），因为基于logits的方法无法传递有关定位的知识。此外，我们在补充材料中提供了有关如何调整β的直观指导。然而，蒸馏性能与β之间的严格相关性尚未得到充分研究，这将成为我们未来研究的方向。 最后Decoupled Knowledge Distillation (DKD). 解耦知识蒸馏。 https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Decoupled_Knowledge_Distillation_CVPR_2022_paper.pdf","categories":[{"name":"CVPR","slug":"CVPR","permalink":"http://example.com/categories/CVPR/"}],"tags":[{"name":"知识蒸馏","slug":"知识蒸馏","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"},{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"多模态解耦","slug":"多模态解耦","permalink":"http://example.com/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A7%A3%E8%80%A6/"}]},{"title":"Diploma project","slug":"Diploma-project","date":"2023-10-20T15:36:35.000Z","updated":"2023-11-12T14:03:55.477Z","comments":true,"path":"2023/10/20/Diploma-project/","link":"","permalink":"http://example.com/2023/10/20/Diploma-project/","excerpt":"","text":"Abstract Today, with the rapid development of science and technology, virtual reality technology is no longer a strange subject, virtual reality technology has become the focus of domestic and foreign scientific research. The data glove is the most important device among these intelligent wearable devices in virtual reality. The data glove can not only accurately capture and collect the gesture information of human hands, but also timely process and transfer the gesture information to the upper computer, which is conducive to providing visual feedback. However, many data gloves on the market have problems such as low attitude sensor response rate, poor attitude signal acquisition effect, lack of stability, and high cost. This paper studies and designs a virtual reality hand motion capture system based on a flexible bending sensor and MPU-6050. The main contents include the following points: In this research, a flexible sensor was obtained by adding conductive carbon particles into polyester resin with a unique micro-crack structure after the polyester resin film formed. The finger bending deformation leads to an increase in the distance between carbon conductive particles, which reduces the conductivity of the bending sensor and leads to the increase of the resistance value of the bending sensor. Thus, by measuring the resistance value of the bending sensor, you can calculate the bending Angle of the finger. In addition, the built-in three-axis accelerometer of the six-axis sensor MPU-6050 can be used to measure the gravity acceleration components in three axial directions, and then the Euler Angle corresponding to the hand posture can be obtained through the relationship of trigonometric function. Since the leading resistance value of the bending sensor is in a linear relationship with the bending Angle, this study first calibrates the leading resistance value of the bending sensor and deduces the linear expression of the leading resistance value of the bending sensor and the bending Angle of the finger through the method of linear fitting. After the partial voltage signal is collected by the bending sensor, it enters the ADC port of the self-designed Arduino microcontroller. According to Kirchhoff’s law, the Angle of finger bending can be calculated using the voltage value collected. In addition, in order to meet the power supply requirements of different peripherals, this design also adopts the boost chip HX3242 and the buck chip XC6206 to design the lifting voltage circuit, which can make the system run statically. According to the IIC protocol, the three-axis gravity acceleration and quaternion of MPU-6050 are read respectively under the single chip computer program, and the Kalman filter algorithm of the DMP library is used to convert them into corresponding Euler Angle. The calculated finger angles and hand Euler Angle are formatted and encapsulated, and the Bluetooth module sends the data to the upper computer for analysis and posture display. The upper computer adopts the Unity3D visualization engine, which can give users clear and beautiful visual feedback by importing arm FBX files and a self-designed virtual reality environment. Unlike many data gloves in the market, although they can capture hand posture, they are expensive and difficult to wear. This design uses a low-cost flexible sensor and MPU-6050, which can achieve the same capture effect, and is small and easy to wear, which can greatly improve the popularity of data gloves. Key words: Data glove, Flexible sensor, MPU-6050, Unity3D","categories":[{"name":"硬件","slug":"硬件","permalink":"http://example.com/categories/%E7%A1%AC%E4%BB%B6/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"Unity3D","slug":"Unity3D","permalink":"http://example.com/tags/Unity3D/"},{"name":"Arduino","slug":"Arduino","permalink":"http://example.com/tags/Arduino/"}],"author":"YutianLi"},{"title":"Hello World","slug":"hello-world","date":"2023-10-18T12:14:30.001Z","updated":"2023-10-18T12:14:30.001Z","comments":true,"path":"2023/10/18/hello-world/","link":"","permalink":"http://example.com/2023/10/18/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"CVPR","slug":"CVPR","permalink":"http://example.com/categories/CVPR/"},{"name":"硬件","slug":"硬件","permalink":"http://example.com/categories/%E7%A1%AC%E4%BB%B6/"}],"tags":[{"name":"知识蒸馏","slug":"知识蒸馏","permalink":"http://example.com/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"},{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"多模态解耦","slug":"多模态解耦","permalink":"http://example.com/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A7%A3%E8%80%A6/"},{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"Unity3D","slug":"Unity3D","permalink":"http://example.com/tags/Unity3D/"},{"name":"Arduino","slug":"Arduino","permalink":"http://example.com/tags/Arduino/"}]}